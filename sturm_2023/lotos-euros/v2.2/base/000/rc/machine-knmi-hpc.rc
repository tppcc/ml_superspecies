!#######################################################################
!
! KNMI / HPC server
!
! Machine:
!    bxshnr02.knmi.nl         # 2017-08
!
! ARCHITECTURE
!
!     2 sockets/node
!    14 cores/socket
!     2 threads/core
!
! ENVIRONMENT MODULES
!
!    # compiler suite:
!    module load intel/2017.2.174
!
! USER INSTALLED LIBRARIES
!
!    # prefix of installed libs:
!    export OPT="/nfs/BACKUP/segers/opt"
!
!    # add location of module files:
!    module use "/nfs/BACKUP/segers/modulefiles"
!
!    # libs compiled with Intel suite (netcdf, udunits):
!    module load intel-libs
!    # tools
!    module load makedepf90
!
!#######################################################################


!----------------------------------------------------------------------
! user names
!----------------------------------------------------------------------

! used for output attributes ...

! name of institution:
user.institution             :  KNMI

! extract user long name from environment variable:
user.longname                :  ${USER}


!----------------------------------------------------------------------
! compiler settings
!----------------------------------------------------------------------

! Intel Fortran Compiler
#include base/${my.le.patch}/rc/compiler-intel-17.0.2.rc

! MPI wrapper from Intel suite:
mpi.compiler.fc                     :  mpiifort
mpi.compiler.fc.openmp              :  mpiifort


!----------------------------------------------------------------------
! libraries
!----------------------------------------------------------------------

! (optional) macro to enable udunits library:
!my.udunits.define             :  
my.udunits.define             :  with_udunits1
!my.udunits.define             :  with_udunits2

! (optional) macro to enable linear algebra library:
!my.linalg.define             :  
!my.linalg.define             :  with_lapack
!my.linalg.define             :  with_lapack95
my.linalg.define             :  with_mkl_17

! ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

! NetCDF library:
compiler.lib.netcdf.fflags      : -I${NETCDF_FORTRAN_MPI_HOME}/include
compiler.lib.netcdf.libs        : -L${NETCDF_FORTRAN_MPI_HOME}/lib -lnetcdff -Wl,-rpath -Wl,${NETCDF_FORTRAN_MPI_HOME}/lib \
                                  -L${NETCDF_MPI_HOME}/lib -lnetcdf -Wl,-rpath -Wl,${NETCDF_MPI_HOME}/lib

! UDUnits library :
compiler.lib.udunits1.fflags    : -I${UDUNITS_HOME}/include
compiler.lib.udunits1.libs      : -L${UDUNITS_HOME}/lib -ludunits -Wl,-rpath -Wl,${UDUNITS_HOME}/lib

! MPI library:
compiler.lib.mpi.fflags         : 
compiler.lib.mpi.libs           :  

!
! Math-Kernel-Library, includes BLAS, LAPACK
!
! Use the "link-line-advisor" for the required flags:
!   https://software.intel.com/en-us/articles/intel-mkl-link-line-advisor
! Settings:
!   Intel product       :  Intel MKL 2017.0
!   OS                  :  Linux
!   Intel Xeon model    :  None
!   Compiler            :  Intel Fortran
!   Architecture        :  Intel 64
!   Linking             :  dynamic
!   Interface layer     :  32-bit integer
!   Threading layer     :  sequential   |   OpenMP threading
!   Fortan95 interfaces :  [x] LAPACK95
!   Link explicitly     :  [x]
!
! switch:
#if "${par.openmp}" in ["True"]

! threaded version:
compiler.lib.mkl.fflags         :  -I${MKLROOT}/include/intel64/lp64
compiler.lib.mkl.libs           :  -L${MKLROOT}/lib/intel64 \
                                   -lmkl_lapack95_lp64 -lmkl_intel_lp64 \
                                   -lmkl_intel_thread -lmkl_core -liomp5 -lpthread -lm -ldl \
                                   -Wl,-rpath -Wl,${MKLROOT}/lib/intel64

#elif "${par.openmp}" in ["False"]

! sequential version:
compiler.lib.mkl.fflags         :  -I${MKLROOT}/include/intel64/lp64
compiler.lib.mkl.libs           :  -L${MKLROOT}/lib/intel64 \
                                   -lmkl_lapack95_lp64 -lmkl_intel_lp64 \
                                   -lmkl_sequential -lmkl_core -lpthread -lm -ldl \
                                   -Wl,-rpath -Wl,${MKLROOT}/lib/intel64

#else
#error could not configure mkl for par.openmp "${par.openmp}"
#endif


!----------------------------------------------------------------------
! makedep
!----------------------------------------------------------------------

! Is makedepf90 installed?
! This flag is used in the 'expert.rc' settings:
my.with.makedep            :  True


!----------------------------------------------------------------------
! maker
!----------------------------------------------------------------------

! default number of build jobs that can be used,
! this is passed to 'build.jobs' in the expert rc:
!my.build.jobs       :  1
!~ unlimitted:
my.build.jobs       : 

! make command; 
! the setup script will insert the 'build.jobs' specified in the expert.rc
! or passed as argument to the setup script:
!
maker               :  gmake -j %{build.jobs}

!----------------------------------------------------------------------
! MPI runner
!----------------------------------------------------------------------

!~ Intel suite:
!  -n <ntask>                : number of tasks
!  -outfile-pattern <file>   : writes task output to file, include '%r' for process rank
!  -errfile-pattern <file>   : writes task output to file, include '%r' for process rank
!mpirun.command     :  mpirun
!mpirun.args        :  -n ${par.ntask} -outfile-pattern ${job.name}.out.%r -errfile-pattern ${job.name}.err.%r

!~ OpenMPI wrapper:
!  --np <ntask>              : number of processes ("tasks")
!  --output-filename <file>  : writes task output to "<file>.<node>.<task>"
!mpirun.command     :  mpirun
!mpirun.args        :  --np ${par.ntask} --map-by :PE=${my.run.nthread} \
!                        --display-devel-map --display-allocation \
!                        --output-filename ${job.name}.out

! SLURM: use 'srun' instead of 'mpirun'.
! Seems necessary to export explicitly some environment variables.
mpirun.command     :  srun
mpirun.args        :  --nodes=1 \
                       --ntasks=${par.ntask} \
                       --cpus-per-task=${my.run.nthread} \
                       --export=LD_LIBRARY_PATH,UDUNITS_PATH,OMP_NUM_THREADS=${my.run.nthread} \
                       --output=${job.name}_%{step}.out.%t --error=${job.name}_%{step}.err.%t

! name of command file; if empty, then executable and arguments are added to the command line
mpirun.cmdfile     :  

! name of host file:
mpirun.hostfile    : 


!----------------------------------------------------------------------
! debugger
!----------------------------------------------------------------------

! debugger type: totalview | idb | kdbg
debugger                :  idb

! command for debugger:
debugger.command        :  idb


!----------------------------------------------------------------------
! runner
!----------------------------------------------------------------------

! special runner command ? e.g. :
!   dplace -s1 test.x argsdebugger type: totalview | idb | kdbg
runner.command          :  
!runner.command          :  dplace -s1


!----------------------------------------------------------------------
! jobs
!----------------------------------------------------------------------

! where to submit jobs to by default ?
my.submit.to.default               : queue

! allowed destinations:
my.submit.to.allowed               : foreground queue

! shell to use in job scripts:
! o python scripts:
job.shell.python                   : /usr/bin/env python


!----------------------------------------------------------------------
! some run time settings ...
!----------------------------------------------------------------------

! number of threads in run step:
#if "${par.openmp}" in ["yes","T","True"] :
my.run.nthread                    :  ${par.nthread}
#elif "${par.openmp}" in ["no","F","False"] :
my.run.nthread                    :  1
#else
#error Could not set number of OpenMP threads for par.openmp "${par.openmp}"
#endif

! number of threads in run step:
#if "${par.mpi}" in ["yes","T","True"] :
my.run.ntask                      :  ${par.ntask}
#elif "${par.mpi}" in ["no","F","False"] :
my.run.ntask                      :  1
#else
#error Could not set number of MPI tasks for par.openmp "${par.mpi}"
#endif

!! total number:
!my.run.ncpu          :  $(( ${my.run.nthread} * ${my.run.ntask} ))


!----------------------------------------------------------------------
! shell options
!----------------------------------------------------------------------

! for running in foreground or background;
! add python code that is directly inserted in header

shell.options.default       :

shell.options.init          :  

shell.options.run           :  \n\
# machine specific settings:\n\
import os\n\
os.putenv( 'LSB_STDOUT_DIRECT', '1' )\n\
os.putenv( 'OMP_NUM_THREADS', '${par.nthread}' )

shell.options.done          :


!---------------------------------------------------------------------------------------------------
! Settings for SLURM job manager - for details and other options: "man sbatch" or "web documentation" 
!---------------------------------------------------------------------------------------------------

! queue type:
queue :  slurm

! list of queue options (same for each step):
queue.slurm.options                 :  job-name qos partition nodes ntasks cpus-per-task output error

! job name:
queue.slurm.option.job-name         :  ${job.name}

! partition
queue.slurm.option.partition        :  all

! quality-of-service, here queue (account) name:
queue.slurm.option.qos              :  proj-cams50

! minimum number of node
queue.slurm.option.nodes            :  1

! maximum number of tasks:
queue.slurm.option.ntasks           :  ${my.run.ntask}

! maximum number of tasks:
queue.slurm.option.cpus-per-task    :  ${my.run.nthread}

! log files:
queue.slurm.option.output           :  <auto>
queue.slurm.option.error            :  <auto>

! adhoc, no idea why this is needed by scripting ..
queue.slurm.args        : 
queue.slurm.args.init   : 
queue.slurm.args.run    : 
queue.slurm.args.done   : 


!----------------------------------------------------------------------
! model data
!----------------------------------------------------------------------

! the user scratch directory:
!my.scratch                    :  /scratch/${USER}
#if "${USER}" == "segers"
my.scratch                    :  /lustre3/projects/CAMS50/${USER}/scratch
#else
my.scratch                    :  /lustre3/projects/LEUpdate/${USER}/scratch
#endif

! base path to input data files:
my.data.dir                   :  /lustre3/projects/CAMS50/${USER}/data

!! extra install tasks:
!my.install.tasks              :

